{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0d48c7bdae873f1c2f85b053c005cb0467203ab6c325afd72fd70f1bdbbf87920",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tuning A Neural Network Using SIF vs KF (Classification Task)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: filterpy in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (1.4.5)\n",
      "Requirement already satisfied: scipy in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from filterpy) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from filterpy) (3.3.4)\n",
      "Requirement already satisfied: numpy in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from filterpy) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->filterpy) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->filterpy) (7.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->filterpy) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->filterpy) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->filterpy) (2.4.6)\n",
      "Requirement already satisfied: six in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->filterpy) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/onursurucu/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->filterpy) (46.0.0.post20200309)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/Users/onursurucu/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries\n",
    "!pip install filterpy\n",
    "\n",
    "# Importing global modules\n",
    "from pprint import pformat\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from filterpy.kalman import (\n",
    "    KalmanFilter,\n",
    "    UnscentedKalmanFilter,\n",
    "    MerweScaledSigmaPoints,\n",
    "    unscented_transform,\n",
    ")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# import matlab.engine\n",
    "from io import StringIO\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# eng = matlab.engine.start_matlab()\n",
    "\n",
    "# Importing local modules\n",
    "import ukf\n",
    "import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking of weight records of every epochs\n",
    "class EpochInfoTracker(Callback):\n",
    "    def __init__(self):\n",
    "        self.weights_history = []  # Tracking the weights in each epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        weights_vec = get_weights_vector(self.model)\n",
    "        self.weights_history.append(weights_vec)\n",
    "\n",
    "\n",
    "# Class for storing the necessary parameters\n",
    "class Params:\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## Loading Iris Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X dataset shape: (150, 4)\ny dataset shape: (150, 3)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()  # Load iris dataset\n",
    "\n",
    "# Create X and y of dataframe\n",
    "X = iris.data[:, :4]  # X dataset\n",
    "y = np.asarray(pd.get_dummies(iris.target))  # y dataset\n",
    "\n",
    "print(\"X dataset shape:\", X.shape)\n",
    "print(\"y dataset shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train shape: (105, 4)\ny_train shape: (105, 3)\nX_test shape : (45, 4)\ny_test shape : (45, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "print(\"y_test shape :\", y_test.shape)"
   ]
  },
  {
   "source": [
    "## Initialize Essential Functions and Parameters for the Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------Initialization of the parameters-----------------\n",
    "params = Params()\n",
    "params.epochs = 1200\n",
    "params.train_series_length = X_train.shape[0]\n",
    "params.test_series_length = X_test.shape[0]\n",
    "params.mg_tau = 30\n",
    "# params.window_size = 12    # M\n",
    "params.ukf_dt = 0.1\n",
    "params.alpha, params.beta, params.kappa = 1, 2, 1  # Worked well\n",
    "# params.alpha, params.beta, params.kappa = 0.001, 2, 1\n",
    "params.Q_var = 0.001\n",
    "params.R_var = 0.001\n",
    "\n",
    "# To make training data and related variables accessible across functions\n",
    "params.train_ukf_ann = True\n",
    "params.X_data = None\n",
    "params.y_data = None\n",
    "params.hxw_model = None\n",
    "params.curr_idx = 0\n",
    "\n",
    "\n",
    "# ---------------- Initialization of the necessary functions------------------\n",
    "def measurement_func(w, x):\n",
    "    hxw_model = params.hxw_model\n",
    "    qq = np.asarray(w)\n",
    "    ww = np.reshape(qq, -1)\n",
    "    set_weights(hxw_model, ww)\n",
    "    # Reshape needed to feed x as 1 sample to ANN model\n",
    "    hxw = hxw_model.predict(x.reshape(1, len(x)))\n",
    "    hxw = hxw.flatten()  # Flatten to make shape = (1,)\n",
    "\n",
    "\n",
    "# Create ukf using pykalman library\n",
    "def create_ukf(Q, R, dt, w_init, P_init):\n",
    "    M = w_init.shape[0]\n",
    "\n",
    "    points = MerweScaledSigmaPoints(M, params.alpha, params.beta, params.kappa)\n",
    "\n",
    "    ukf = UnscentedKalmanFilter(dim_x=M, dim_z=1, dt=dt, fx=fw, hx=hw, points=points)\n",
    "    ukf.x = w_init\n",
    "    ukf.P = P_init\n",
    "    ukf.R = R\n",
    "    ukf.Q = Q\n",
    "\n",
    "    return ukf\n",
    "\n",
    "\n",
    "# Create ukf instance using ukf.py (custom ukf)\n",
    "def create_my_ukf(Q, R, dt, w_init, P_init):\n",
    "    my_ukf = ukf.UnscentedKalmanFilter(\n",
    "        fw, hw, R, Q, w_init, P_init, params.alpha, params.beta, params.kappa\n",
    "    )\n",
    "    return my_ukf\n",
    "\n",
    "\n",
    "# Function for Kalman filter\n",
    "def fw(w, dt=None):\n",
    "    return w  # Identity\n",
    "\n",
    "\n",
    "# Function for Kalman filter\n",
    "def hw(w):\n",
    "    x = params.X_data[params.curr_idx]\n",
    "    hxw = measurement_func(w, x)\n",
    "    return hxw\n",
    "\n",
    "\n",
    "def evaluate_neural_nets(\n",
    "    sgd_ann, ukf_ann, window, use_train_series=False, train_series=None\n",
    "):\n",
    "    if use_train_series:\n",
    "        X_data, y_data = X_train, y_train\n",
    "        series = train_series\n",
    "        sample_len = params.train_series_length\n",
    "        title = \"Train series (true vs. predicted)\"\n",
    "    else:\n",
    "        sample_len = params.test_series_length\n",
    "        X_data, y_data = X_test, y_test\n",
    "        title = \"Test series (true vs. predicted)\"\n",
    "\n",
    "    sgd_pred, sgd_self_pred = utility.predict_series(\n",
    "        sgd_ann, X_data, sample_len, window\n",
    "    )\n",
    "    ukf_pred, ukf_self_pred = utility.predict_series(\n",
    "        ukf_ann, X_data, sample_len, window\n",
    "    )\n",
    "\n",
    "    utility.plot(range(sample_len), series, title=title, label=\"True series\")\n",
    "    # utility.plot(range(sample_len), sgd_pred, new_figure=False, label='SGD ANN prediction (based on true windows)')\n",
    "    utility.plot(\n",
    "        range(sample_len), ukf_pred, new_figure=False, label=\"SIF ANN prediction\"\n",
    "    )\n",
    "    if not use_train_series:\n",
    "        preds = ukf_ann.predict(X_data)\n",
    "        accuracy = accuracy_score(y_data, preds)\n",
    "        print(\"The Test accuracy is: \", accuracy)\n",
    "\n",
    "    # utility.plot(range(sample_len), y_self_pred_series, new_figure=False,\n",
    "    #              label='Predicted test series (rolling prediction: no true vals used)')\n",
    "\n",
    "\n",
    "# Create a simple feedforward neural network\n",
    "def create_neural_net(M):\n",
    "\n",
    "    # Build a simple neural network\n",
    "    ann = Sequential()\n",
    "    ann.add(Dense(1, input_dim=M, activation=\"relu\"))\n",
    "    ann.add(Dense(3, activation=\"softmax\"))\n",
    "    ann.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=\"accuracy\")\n",
    "\n",
    "    # Print out the summary of the model\n",
    "    ann.summary()\n",
    "\n",
    "    return ann\n",
    "\n",
    "\n",
    "# Get weights of the neural network model\n",
    "def get_weights_vector(model):\n",
    "    weights = model.get_weights()\n",
    "    # logging.info(weights)\n",
    "    weights_vec = []\n",
    "    for w_mat in weights:\n",
    "        weights_vec.extend(w_mat.reshape(w_mat.size))\n",
    "\n",
    "    weights_vec = np.array(weights_vec)\n",
    "    return weights_vec\n",
    "\n",
    "\n",
    "# Set weights of the neural network model\n",
    "def set_weights(model, weights_vec):\n",
    "    prev_weights = model.get_weights()\n",
    "    # logging.info(prev_weights)\n",
    "    new_weights = []\n",
    "    start = 0\n",
    "\n",
    "    for prev_w_mat in prev_weights:\n",
    "        end = start + prev_w_mat.size\n",
    "        new_w_mat = np.array(weights_vec[start:end]).reshape(prev_w_mat.shape)\n",
    "        new_weights.append(new_w_mat)\n",
    "        start = end\n",
    "\n",
    "    model.set_weights(new_weights)\n",
    "\n",
    "\n",
    "def test_weights_functions():\n",
    "    ann = create_neural_net(10)\n",
    "    prev_weights = ann.get_weights()\n",
    "    vec = get_weights_vector(ann)\n",
    "    # vec = [elem + 1 for elem in vec]\n",
    "\n",
    "    ann2 = create_neural_net(10)\n",
    "    set_weights(ann2, vec)\n",
    "    post_weights = ann2.get_weights()\n",
    "\n",
    "    for w_mat1, w_mat2 in zip(prev_weights, post_weights):\n",
    "        assert np.array_equal(w_mat1, w_mat2)\n",
    "\n",
    "    logging.info(prev_weights)\n",
    "    logging.info(post_weights)"
   ]
  },
  {
   "source": [
    "## Main"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 6         \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 6         \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 6         \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 6         \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "The accuracy is:  0.0\n",
      "The accuracy is:  0.4380952380952381\n",
      "The accuracy is:  0.4380952380952381\n",
      "The accuracy is:  0.5904761904761905\n",
      "The accuracy is:  0.5904761904761905\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n",
      "The accuracy is:  0.6761904761904762\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9719d577aa84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # This line disables GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9719d577aa84>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# SIF Predicition Stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             predict_genetic = ukf_ann.predict(\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             )  # Perform the prediction with given weights\n\u001b[1;32m    133\u001b[0m             predict_genetic = np.max(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \"\"\"\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4206\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4207\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4208\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4209\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3211\u001b[0m         \u001b[0;31m# places (like Keras) where the FuncGraph lives longer than the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m         \u001b[0;31m# ConcreteFunction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m         shared_func_graph=False)\n\u001b[0m\u001b[1;32m   3214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[0;31m# FuncGraph directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0;32m-> 1556\u001b[0;31m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[1;32m   1557\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    614\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[1;32m    615\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[1;32m    617\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    471\u001b[0m       \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mfunction_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mfunction_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Setting parameters\n",
    "\n",
    "    # Known paramaters are hx function (neural net), Q, R, w_init\n",
    "    # No. of state variables = no. of weights in neural net\n",
    "    # No. of measurement variables = D = 1 (y)\n",
    "\n",
    "    dt = 0.01  # Setting learning rate\n",
    "    n_samples = params.train_series_length                      # Setting training series length\n",
    "\n",
    "    # Create ANN, get its initial weights\n",
    "    params.hxw_model = create_neural_net(X_train.shape[1])      # Create a neural net model\n",
    "    w_init = get_weights_vector(params.hxw_model)               # Get weights from neural nets\n",
    "    num_weights = w_init.shape[0]                               # Number of weights inside the neural network\n",
    "\n",
    "    # ---------------------------Filter Parameters-----------------------\n",
    "\n",
    "    # -----------------UKF Parameter------------------\n",
    "    P_init = 0.1 * np.eye(num_weights)                          # Initial values of covariance matrix of state variables (MxM)\n",
    "    Q = params.Q_var * np.eye(num_weights)                      # Process noise covariance matrix (MxM)\n",
    "    R = np.array([[params.R_var]])                              # Measurement noise covariance matrix (DxD)\n",
    "\n",
    "    sgd_ann = create_neural_net(X_train.shape[1])               # Create neural network model\n",
    "    sgd_ann.set_weights(params.hxw_model.get_weights())         # Set the weights for neural network (Same starting point as the UKF_ANN)\n",
    "\n",
    "    ukf_ann = create_neural_net(X_train.shape[1])               # Create neural network for ukf\n",
    "    testann = create_neural_net(X_train.shape[1])\n",
    " \n",
    "    ukf_ann.set_weights(params.hxw_model.get_weights())         # Set the ukf weighst same as sgd_nn (Same starting point as the UKF_ANN)\n",
    "\n",
    "    z_true_series = y_train                                     # Set the test set as the training set\n",
    "    num_iter = params.epochs * len(z_true_series)               # Initialize max_iteration: epochs * dataset_len\n",
    "\n",
    "    # 2 Kalman filter implementations to compare (from filterpy and my custom impl)\n",
    "\n",
    "    ukf_filter = create_ukf(Q, R, dt, w_init, P_init)           # Initialization of the UKF\n",
    "    my_ukf = create_my_ukf(Q, R, dt, w_init, P_init)            # Initialiation of the SIF\n",
    "\n",
    "    # Pre-allocate output variables\n",
    "    ukf_w = np.zeros((num_weights, params.epochs))\n",
    "    my_ukf_w = np.zeros((num_weights, params.epochs))\n",
    "    ukf_train_accuracy = np.zeros(params.epochs)\n",
    "    my_ukf_train_accuracy = np.zeros(params.epochs)\n",
    "    sgd_train_mse = np.zeros(params.epochs)\n",
    "\n",
    "    # -----------SIF Initalize Variables--------------\n",
    "    x = w_init                                                  # Weights of the neural network\n",
    "    n = x.shape[0]                                              # Number of States\n",
    "    m = z_true_series.shape[0]                                  \n",
    "    # delta = [[0.09], [9], [0.9]]\n",
    "    delta = np.random.uniform(low=0.0009, high=0.9, size=(X_train.shape[0]))\n",
    "    sat = np.zeros((m, m))\n",
    "    C = np.ones((X_train.shape[0], n))\n",
    "    P = P_init\n",
    "    innovA = np.zeros((m, 1))\n",
    "\n",
    "    N = len(x)\n",
    "    w = np.zeros((x.shape))\n",
    "    eta = 0.01\n",
    "    x = get_weights_vector(ukf_ann)\n",
    "    pdiff = np.zeros((num_iter, 1))\n",
    "    # -------------------------------------------\n",
    "    # Train SGD ANN (for comparison)\n",
    "    logging.info(\"Training neural net with SGD\")\n",
    "    info_tracker = EpochInfoTracker()\n",
    "    callbacks = [info_tracker]\n",
    "    history = sgd_ann.fit(\n",
    "        X_train, y_train, batch_size=1, epochs=1, verbose=3, callbacks=callbacks,\n",
    "    )\n",
    "    logging.info(\"Training SGD complete\")\n",
    "    # -------------------------------------------\n",
    "    # Training loop with UKF\n",
    "    out = StringIO()\n",
    "    sifnn = []\n",
    "    logging.info(\"Training neural net with UKF\")\n",
    "    t0 = time.time()\n",
    "    epoch = 0\n",
    "    # num_iter = 10 #hack\n",
    "    minval = np.ones((num_iter, 1))\n",
    "    aRate = 0.5\n",
    "\n",
    "    # Epochs * len(y_train)\n",
    "    for i in range(num_iter):\n",
    "        # print(\"SHOUD\", mean_squared_error(z_true_series, ukf_ann.predict(params.X_data)))\n",
    "        idx = i % len(z_true_series)\n",
    "        # logging.info(idx)\n",
    "        if 0 == 0:\n",
    "            if not params.train_ukf_ann:\n",
    "                break\n",
    "            # Checking the accuracy of the model\n",
    "            preds_softmax = ukf_ann.predict(X_train)                            # Model prediction (softmax format)\n",
    "            z_true_series_accuracy = np.argmax(z_true_series, axis=1)           # Select the highest probability as the output\n",
    "            preds_accuracy = np.argmax(preds_softmax, axis=1)                   # Take the highest possibility as output among softmax output\n",
    "            accuracy = accuracy_score(z_true_series_accuracy, preds_accuracy)   # Calculate the accuracy\n",
    "\n",
    "            ukf_train_accuracy[epoch] = accuracy\n",
    "            #my_ukf_train_accuracy[epoch] = accuracy\n",
    "            sifnn.append(accuracy)\n",
    "            print(\"The accuracy is: \", accuracy)\n",
    "            if (accuracy >= 0.8) and (i > 1):\n",
    "                thelast = i\n",
    "                break\n",
    "            # ukf_w[:, epoch] = x[:]\n",
    "            # my_ukf_w[:, epoch] = x[:]\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "        # -----------------Genetic Algorithm------------\n",
    "        accuracy_GA = []\n",
    "        weights_GA = []\n",
    "        for jj in range(100):\n",
    "            weights_GA.append(get_weights_vector(ukf_ann))                      # Store the weights vector\n",
    "            accuracy_GA.append(accuracy)                                        # Store the accuracy values\n",
    "            params.curr_idx = (\n",
    "                idx                                                             # For use in hw() to fetch correct x_k sample\n",
    "            )\n",
    "            z = z_true_series[idx]\n",
    "\n",
    "            ##################################################\n",
    "            # SIF Predicition Stage\n",
    "            predict_genetic = ukf_ann.predict(\n",
    "                X_train\n",
    "            )                                                                   # Perform the prediction with given weights\n",
    "            predict_genetic = np.max(\n",
    "                predict_genetic, axis=1\n",
    "            )                                                                   # Select the highest softmax output\n",
    "            z_max = np.max(z_true_series, axis=1)\n",
    "\n",
    "            innov = z_max - predict_genetic                                     # Set the innovation matrix \n",
    "            x = get_weights_vector(ukf_ann) + aRate * np.sign(\n",
    "                z_max[idx] - predict_genetic[idx]\n",
    "            )                                                                   # Change the weights of the neurons by adding aRate\n",
    "            # innov = z_true_series - np.dot(C,x)\n",
    "            # print(\"Innov \", innov)\n",
    "\n",
    "            ### Do something with delta\n",
    "\n",
    "            ############## CHANGE #################\n",
    "            for i in range(1, m):\n",
    "                # innovA[i] = sum(innov[i])/len(innov[i])\n",
    "                if (abs(innov[i]) / delta[i]) >= 1:\n",
    "                    sat[i][i] = 1\n",
    "                else:\n",
    "                    sat[i][i] = abs(innov[i]) / delta[i]\n",
    "            ######################\n",
    "\n",
    "            pinvC = np.linalg.pinv(C)\n",
    "            K = np.dot(pinvC, sat)\n",
    "            x = np.asarray(\n",
    "                [xx * random.uniform(0.001, 1) for xx in x]\n",
    "            )  # Randomly initialize weights\n",
    "            # print(x.shape, K.shape, innovA.shape, x.shape, np.dot(K, innovA).shape)\n",
    "            was = np.reshape(np.dot(K, innov), x.shape)\n",
    "            x = x + was  # NEED TO CHECK THIS LINE\n",
    "\n",
    "            # print(\"Error \" ,z_true_series[idx] - ukf_ann.predict(params.X_data)[idx])\n",
    "            # print(\"ErrorA \" ,z_true_series[idx] - sgd_ann.predict(params.X_data)[idx])\n",
    "\n",
    "            set_weights(params.hxw_model, x)\n",
    "            set_weights(ukf_ann, x)\n",
    "\n",
    "            preds = ukf_ann.predict(X_train)\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "            accuracy = accuracy_score(z_true_series_accuracy, preds)\n",
    "\n",
    "            # print(\"Accuracy: \", accuracy)\n",
    "            minval[idx] = accuracy\n",
    "            pdiff[idx] = accuracy\n",
    "            # if accuracy <= 0.03:\n",
    "            #   break\n",
    "            # ukf_filter.x = x\n",
    "            # print(type(preds))\n",
    "            accuracy_GA.append(accuracy)\n",
    "            weights_GA.append(x)\n",
    "            # print(accuracy_GA)\n",
    "\n",
    "            # Genetic Algorithm\n",
    "            if jj == 99:\n",
    "                # print(accuracy_GA[accuracy_GA.index(min(accuracy_GA))])\n",
    "\n",
    "                set_weights(ukf_ann, weights_GA[accuracy_GA.index(max(accuracy_GA))])\n",
    "                preds = ukf_ann.predict(X_train)\n",
    "                #     print(\"Last Set: \", mean_squared_error(z_true_series, preds))\n",
    "                set_weights(ukf_ann, weights_GA[accuracy_GA.index(max(accuracy_GA))])\n",
    "        #     print(\"jjj\",mean_squared_error(z_true_series, ukf_ann.predict(params.X_data) ))\n",
    "\n",
    "    time_to_train = time.time() - t0\n",
    "    logging.info(\n",
    "        \"Training complete. time_to_train = {:.2f} sec, {:.2f} min\".format(\n",
    "            time_to_train, time_to_train / 60\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # This line disables GPU\n",
    "    main()\n"
   ]
  },
  {
   "source": [
    "## Result Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # -------------------------------------------\n",
    "    # Results analysis\n",
    "\n",
    "    # Visualize evolution of ANN weights\n",
    "\n",
    "    # Visualize error curve (SGD vs UKF)\n",
    "    x_var = range(thelast + 1)\n",
    "    hist = history.history[\"loss\"]\n",
    "    ukf_train_mse = np.array(sifnn)\n",
    "    # utility.plot(x_var, hist, xlabel='Epoch',\n",
    "    #            label='SGD ANN training history (MSE)')\n",
    "    utility.plot(\n",
    "        x_var, ukf_train_mse, new_figure=False, label=\"SIF ANN training history (MSE)\"\n",
    "    )\n",
    "\n",
    "    # True test series vs. ANN pred vs, UKF pred\n",
    "    logging.info(\"Evaluating and visualizing neural net predictions\")\n",
    "    evaluate_neural_nets(\n",
    "        sgd_ann, ukf_ann, window, use_train_series=True, train_series=X_series\n",
    "    )\n",
    "    evaluate_neural_nets(sgd_ann, ukf_ann, window)\n",
    "\n",
    "    utility.save_all_figures(\"output\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"The Min MSE is \", min(minval), \" vs \", hist[-1])\n",
    "    print(\"Total amount of epochs for SIF: \", epoch)"
   ]
  },
  {
   "source": [
    "## SIF (Step by step)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Initialization of neural networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1)                 5         \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 6         \n=================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (None, 1)                 5         \n_________________________________________________________________\ndense_3 (Dense)              (None, 3)                 6         \n=================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dt = 0.01                                               # Setting learning rate\n",
    "n_samples = params.train_series_length                  # Setting batch size\n",
    "\n",
    "# Create ANN, get its initial weights\n",
    "params.hxw_model = create_neural_net(X_train.shape[1])  # Create a neural net model\n",
    "w_init = get_weights_vector(params.hxw_model)           # Get weights from neural nets\n",
    "num_weights = w_init.shape[0]                           # Number of weights inside the neural network\n",
    "\n",
    "sif_ann = create_neural_net(X_train.shape[1])\n",
    "\n",
    "z_true_series = y_train\n",
    "\n",
    "num_iter = params.epochs * len(z_true_series)"
   ]
  },
  {
   "source": [
    "Getting and setting initial SIF parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------SIF Initalize Variables--------------\n",
    "x = w_init\n",
    "n = x.shape[0]  # Number of States\n",
    "m = z_true_series.shape[0]\n",
    "# delta = [[0.09], [9], [0.9]]\n",
    "delta = np.random.uniform(low=0.0009, high=0.9, size=(X_train.shape[0]))\n",
    "sat = np.zeros((m, m))\n",
    "C = np.ones((X_train.shape[0], n))\n",
    "#P = P_init\n",
    "innovA = np.zeros((m, 1))\n",
    "\n",
    "N = len(x)\n",
    "w = np.zeros((x.shape))\n",
    "eta = 0.01\n",
    "x = get_weights_vector(ukf_ann)\n",
    "pdiff = np.zeros((num_iter, 1))"
   ]
  },
  {
   "source": [
    "Perform prediction with the Neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Training neural net with SGD\")\n",
    "info_tracker = EpochInfoTracker()\n",
    "callbacks = [info_tracker]\n",
    "history = ukf_ann.fit(\n",
    "    X_train, y_train, batch_size=1, epochs=1, verbose=3, callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "preds_softmax = ukf_ann.predict(X_train)            \n",
    "\n",
    "\n",
    "z_true_series_accuracy = np.argmax(z_true_series, axis=1)\n",
    "preds_accuracy = np.argmax(\n",
    "    preds_softmax, axis=1\n",
    ")  # Take the highest possibility as output among softmax output\n",
    "accuracy = accuracy_score(z_true_series_accuracy, preds_accuracy)\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "## Prediction Stage  \n",
    "\n",
    "SIF parameter\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Predicted innovation ($ \\hat{z}_{k+1|k} $): pred_innov\n",
    "\n",
    "True values ($ z_{k+1|k} $):  z_true_series  \n",
    "\n",
    "Measurement matrix ($ \\hat{x}_{k+1|k} $): prediction\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Eq.(3.13):  $ \\hat{z}_{k+1|k}  =  z_{k+1|k}  - C  \\hat{x}_{k+1|k} $ \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predict_genetic' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6b2f66f7801a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mukf_ann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_genetic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_genetic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Select the highest softmax output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mz_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_true_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_innov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_max\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredict_genetic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_genetic' is not defined"
     ]
    }
   ],
   "source": [
    "preds_softmax = ukf_ann.predict(X_train)\n",
    "predict_genetic = np.max(predict_genetic, axis=1)  # Select the highest softmax output\n",
    "z_max = np.max(z_true_series, axis=1)\n",
    "\n",
    "pred_innov = z_max - (C *predict_genetic)\n",
    "print(pred_innov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_weights_vector(ukf_ann) + aRate * np.sign(\n",
    "    z_max[idx] - predict_genetic[idx]\n",
    ")"
   ]
  },
  {
   "source": [
    "Set the saturation term:\n",
    "$K_(k+1) = C^{+} \\overline{sat} (|\\hat{z}_{k+1|k}| / \\delta)$\n",
    "\n",
    "<\\overline{sat}$ refers to the diagonal of the saturation term, sat refers to the saturation of a value (yields a result between 1 and -1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, m):\n",
    "    # innovA[i] = sum(innov[i])/len(innov[i])\n",
    "    if (abs(innov[i]) / delta[i]) >= 1:\n",
    "        sat[i][i] = 1\n",
    "\n",
    "    elif (abs(innov[i]) / delta[i]) <= -1:\n",
    "        sat[i][i] = -1 \n",
    "    else:\n",
    "        sat[i][i] = abs(innov[i]) / delta[i]"
   ]
  }
 ]
}